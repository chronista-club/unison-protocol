use anyhow::Result;
use serde_json::json;
use std::sync::atomic::{AtomicU64, Ordering};
use std::sync::Arc;
use std::time::{Duration, Instant};
use tokio::sync::Barrier;
use tracing::{info, Level};
use unison_protocol::{ProtocolClient, ProtocolServer, UnisonClient, UnisonServer, UnisonServerExt};
use unison_protocol::network::{NetworkError, quic::QuicClient};

/// ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯çµæœ
#[derive(Debug, Clone)]
struct BenchmarkResult {
    /// ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚µã‚¤ã‚ºï¼ˆãƒã‚¤ãƒˆï¼‰
    message_size: usize,
    /// å¹³å‡ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·ï¼ˆãƒã‚¤ã‚¯ãƒ­ç§’ï¼‰
    avg_latency_us: f64,
    /// P50ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·ï¼ˆãƒã‚¤ã‚¯ãƒ­ç§’ï¼‰
    p50_latency_us: f64,
    /// P99ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·ï¼ˆãƒã‚¤ã‚¯ãƒ­ç§’ï¼‰
    p99_latency_us: f64,
    /// ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆï¼ˆãƒ¡ãƒƒã‚»ãƒ¼ã‚¸/ç§’ï¼‰
    throughput_msg_per_sec: f64,
    /// CPUä½¿ç”¨ç‡ï¼ˆï¼…ï¼‰- ç°¡æ˜“æ¸¬å®š
    cpu_usage_percent: f64,
}

/// ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·ã‚’æ¸¬å®š
async fn measure_latency(
    client: &mut ProtocolClient,
    message_size: usize,
    iterations: usize,
) -> Vec<u64> {
    let mut latencies = Vec::with_capacity(iterations);
    let message = json!({
        "data": "x".repeat(message_size),
        "sequence": 0
    });

    for i in 0..iterations {
        let mut msg = message.clone();
        msg["sequence"] = json!(i);
        
        let start = Instant::now();
        let _ = client.call("echo", msg).await;
        let elapsed = start.elapsed().as_micros() as u64;
        latencies.push(elapsed);
    }

    latencies.sort_unstable();
    latencies
}

/// ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆã‚’æ¸¬å®š
async fn measure_throughput(
    client: &mut ProtocolClient,
    message_size: usize,
    duration: Duration,
) -> f64 {
    let message = json!({
        "data": "x".repeat(message_size),
        "sequence": 0
    });
    
    let start = Instant::now();
    let mut count = 0u64;
    let mut sequence = 0u32;
    
    while start.elapsed() < duration {
        let mut msg = message.clone();
        msg["sequence"] = json!(sequence);
        
        if client.call("echo", msg).await.is_ok() {
            count += 1;
            sequence = sequence.wrapping_add(1);
        }
    }
    
    let elapsed = start.elapsed().as_secs_f64();
    count as f64 / elapsed
}

/// CPUä½¿ç”¨ç‡ã‚’ç°¡æ˜“çš„ã«æ¸¬å®š
async fn measure_cpu_usage() -> f64 {
    // ç°¡æ˜“çš„ãªå®Ÿè£…ï¼šãƒ—ãƒ­ã‚»ã‚¹ã®CPUæ™‚é–“ã‚’æ¸¬å®š
    let start_time = std::time::SystemTime::now();
    tokio::time::sleep(Duration::from_millis(100)).await;
    let _elapsed = start_time.elapsed().unwrap();
    
    // å®Ÿéš›ã®CPUä½¿ç”¨ç‡æ¸¬å®šã¯è¤‡é›‘ãªã®ã§ã€ãƒ€ãƒŸãƒ¼å€¤ã‚’è¿”ã™
    // æœ¬ç•ªç’°å¢ƒã§ã¯ sysinfo ã‚¯ãƒ¬ãƒ¼ãƒˆãªã©ã‚’ä½¿ç”¨
    35.0
}

/// ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã‚µãƒ¼ãƒãƒ¼ã‚’èµ·å‹•
async fn start_benchmark_server() -> Result<()> {
    let mut server = ProtocolServer::new();
    let counter = Arc::new(AtomicU64::new(0));
    let counter_clone = counter.clone();
    
    // Echo handler
    server.register_handler("echo", move |payload| {
        counter_clone.fetch_add(1, Ordering::Relaxed);
        Ok(payload) as Result<serde_json::Value, NetworkError>
    });
    
    info!("ğŸ“Š Benchmark server starting on 127.0.0.1:8080");
    server.listen("127.0.0.1:8080").await?;
    
    Ok(())
}

/// ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã‚’å®Ÿè¡Œ
async fn run_benchmark(message_size: usize) -> Result<BenchmarkResult> {
    let quic_client = QuicClient::new();
    let mut client = ProtocolClient::new(quic_client);
    client.connect("127.0.0.1:8080").await?;
    
    info!("ğŸ“ Testing with message size: {} bytes", message_size);
    
    // ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·æ¸¬å®š
    info!("  â±ï¸  Measuring latency...");
    let latencies = measure_latency(&mut client, message_size, 1000).await;
    
    let avg_latency = latencies.iter().sum::<u64>() as f64 / latencies.len() as f64;
    let p50_latency = latencies[latencies.len() / 2] as f64;
    let p99_latency = latencies[latencies.len() * 99 / 100] as f64;
    
    // ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆæ¸¬å®š
    info!("  ğŸ“ˆ Measuring throughput...");
    let throughput = measure_throughput(&mut client, message_size, Duration::from_secs(5)).await;
    
    // CPUä½¿ç”¨ç‡æ¸¬å®š
    info!("  ğŸ’» Measuring CPU usage...");
    let cpu_usage = measure_cpu_usage().await;
    
    client.disconnect().await?;
    
    Ok(BenchmarkResult {
        message_size,
        avg_latency_us: avg_latency,
        p50_latency_us: p50_latency,
        p99_latency_us: p99_latency,
        throughput_msg_per_sec: throughput,
        cpu_usage_percent: cpu_usage,
    })
}

#[tokio::main]
async fn main() -> Result<()> {
    // ãƒ­ã‚®ãƒ³ã‚°è¨­å®š
    tracing_subscriber::fmt()
        .with_max_level(Level::INFO)
        .init();

    info!("ğŸµ Unison Protocol Benchmark");
    info!("=============================");
    
    // ã‚µãƒ¼ãƒãƒ¼ã‚’åˆ¥ã‚¿ã‚¹ã‚¯ã§èµ·å‹•
    let barrier = Arc::new(Barrier::new(2));
    let barrier_clone = barrier.clone();
    
    tokio::spawn(async move {
        let _ = start_benchmark_server().await;
        barrier_clone.wait().await;
    });
    
    // ã‚µãƒ¼ãƒãƒ¼ã®èµ·å‹•ã‚’å¾…ã¤
    tokio::time::sleep(Duration::from_millis(500)).await;
    
    // å„ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚µã‚¤ã‚ºã§ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯å®Ÿè¡Œ
    let message_sizes = vec![64, 256, 1024, 4096, 16384];
    let mut results = Vec::new();
    
    for size in message_sizes {
        match run_benchmark(size).await {
            Ok(result) => {
                results.push(result.clone());
                info!("âœ… Completed benchmark for {} bytes", size);
                info!("   - Avg latency: {:.2} Âµs", result.avg_latency_us);
                info!("   - P50 latency: {:.2} Âµs", result.p50_latency_us);
                info!("   - P99 latency: {:.2} Âµs", result.p99_latency_us);
                info!("   - Throughput: {:.0} msg/s", result.throughput_msg_per_sec);
                info!("   - CPU usage: {:.1}%", result.cpu_usage_percent);
            }
            Err(e) => {
                eprintln!("âŒ Benchmark failed for {} bytes: {}", size, e);
            }
        }
    }
    
    // çµæœã®ã‚µãƒãƒªãƒ¼ã‚’è¡¨ç¤º
    info!("");
    info!("ğŸ“Š Benchmark Summary");
    info!("====================");
    info!("");
    info!("| Message Size | Avg Latency | P50 Latency | P99 Latency | Throughput | CPU Usage |");
    info!("|-------------|-------------|-------------|-------------|------------|-----------|");
    
    for result in &results {
        info!(
            "| {:>11} | {:>9.2} Âµs | {:>9.2} Âµs | {:>9.2} Âµs | {:>7.0} msg/s | {:>7.1}% |",
            format!("{} B", result.message_size),
            result.avg_latency_us,
            result.p50_latency_us,
            result.p99_latency_us,
            result.throughput_msg_per_sec,
            result.cpu_usage_percent
        );
    }
    
    info!("");
    info!("ğŸ“ Markdown Table for README:");
    info!("");
    info!("| ãƒ¡ãƒˆãƒªã‚¯ã‚¹ | 64B | 256B | 1KB | 4KB | 16KB |");
    info!("|-----------|-----|------|-----|-----|------|");
    
    // å¹³å‡ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·è¡Œ
    print!("| å¹³å‡ãƒ¬ã‚¤ãƒ†ãƒ³ã‚· |");
    for result in &results {
        print!(" {:.1}Âµs |", result.avg_latency_us);
    }
    println!();
    
    // P50ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·è¡Œ
    print!("| P50ãƒ¬ã‚¤ãƒ†ãƒ³ã‚· |");
    for result in &results {
        print!(" {:.1}Âµs |", result.p50_latency_us);
    }
    println!();
    
    // P99ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·è¡Œ
    print!("| P99ãƒ¬ã‚¤ãƒ†ãƒ³ã‚· |");
    for result in &results {
        print!(" {:.1}Âµs |", result.p99_latency_us);
    }
    println!();
    
    // ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆè¡Œ
    print!("| ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆ |");
    for result in &results {
        if result.throughput_msg_per_sec > 1000.0 {
            print!(" {:.1}K msg/s |", result.throughput_msg_per_sec / 1000.0);
        } else {
            print!(" {:.0} msg/s |", result.throughput_msg_per_sec);
        }
    }
    println!();
    
    info!("");
    info!("âœ… Benchmark completed!");
    
    // ã‚µãƒ¼ãƒãƒ¼ã‚’çµ‚äº†
    barrier.wait().await;
    
    Ok(())
}